# Is authorship sufficient for today’s collaborative research? A call for contributor roles

## Introduction
### Background perspectives on authorship
Scientific authorship generally consists of publishing academic findings via journal articles, book chapters, and monographs [1]. In academic collaborations within science and engineering where co-authorship is the norm, authorship status is attributed to those who have made a significant contribution to certain tasks within the project [2]. Beyond being used as an instrument to recognize contributions, authorship is also used to hold contributors accountable for the accuracy and integrity of published claims [3].

Receiving recognition through authorship has long been the entrenched currency of the scholarly realm. Even so, it has long been recognized that assigning authorship credit is neither a fair nor uniform process [4]. Historically, concerns about authorship credit expressed by academics in the literature centered around what was seen as an egregious problem of awarding authorship credit to those who did not deserve it, and consequently diminishing the contributions of the first, or primary authors. Terms such as profligate, honorary, and courtesy authorship describe various forms of authorship abuse. Some of the proposed solutions to address these problems include defining criteria for authorship (e.g. by the Vancouver group since 1987), providing details of contributions [5], and assigning a rating to authors’ efforts [6]. These solutions often stemmed from a desire to narrow the criteria for authorship, and to clarify roles or the extent of contributions to prevent awarding author status to those who did not deserve it. Nevertheless, applying these solutions in practice may contribute to other tensions. 

## Contributorship: making the contribution the focus, rather than the resulting paper

### Authorship versus contribution

While the definition and the exact role of an author is somewhat ambiguous, tracking contributorship in publications is intended to more explicitly define and give credit to contributors to a work.
Contributors can contribute to a study and/or publication in various ways, and may not necessarily be involved in the writing or revision of the manuscript.
Traditional roles of contributors may include the planning, conducting, and reporting the work.
Non-traditional roles may be more varied.
For example, a primary technician may perform assays such as human brain autopsies, whole brain hemisphere sectioning, immunohistochemical staining, stereological analysis and and assist with publication preparation.
Or a librarian may deliver invited lectures and served as a consultant for libraries to help them establish clinical and translational research support services.
These non-traditional roles can be essential to the success of scholarship, but are often not credited with authorship as often as more traditional roles.


| Assigning authorship credit can easily go awry, damaging the reputation of authors, institutions, journals and science in general, as exemplified in [7] where a published work was retracted because of an authorship dispute. Ongoing questions also persist across disciplines regarding credit for the staff who performed most, if not all experiments that lead to knowledge and breakthroughs, as demonstrated in the debate on "Who really made Dolly?" in the Guardian [8]: “You get some papers where the authors haven't done a scrap of work themselves, it's all down to the technicians acknowledged at the back.” |
|:-------------------------------------------------------------------------------------------------|

Modern research is interdisciplinary, reflecting a team science approach where the skills needed to conduct reliable science are often specialized [9]. In this dynamic where various contribution-types are required, revamping our understanding of authorship, credit, and recognition of individual efforts in academia seems necessary [10]. Rather than coming from a place of censure, we propose a continuum in which contributions from a team of people could be welcomed and recognized. 

## Challenges of authorship 
### Ethical challenges 
As authorship credit remains the single most important form of recognizing individual contributions, tensions around its definition and enforcement remain challenging to address. Many guidelines such as those provided by the Council of Science Editors [11] and The International Committee of Medical Journal Editors [12] suggest that authors should have made a ‘significant contribution’ to the study. Nevertheless, what constitutes a ‘significant contribution’ is ambiguous and difficult to formally define [13]. Furthermore, while a strict application of suggested authorship criteria within these guidelines may exclude contributors who made important  but non-standard contributions including dataset management, software and protocol development [14], a lax attitude towards authorship criteria might lead to inflated bylines and hyperauthorship [15]. 

While modern science needs the participation of a range of contributors, in recent decades a steady increase in the average number of co-authors per publication [16] has contributed to major ethical issues. For instance, in the presence of more co-authors, addressing ethical challenges in the distribution of authorship, acknowledgment credit [17], and handling authorship order [18]would be more challenging. Similarly, with more authors in the byline, ambiguities in relation to individual and shared responsibilities are much more pronounced [19]. As such, questions about the attribution of authorship status to various contributors remain difficult to answer. For example, it is not clear whether Principal Investigators always deserve authorship status [20], or, should graduate students, research technicians, project/program managers, and core lab scientists be included as authors or only mentioned in the acknowledgment section. Moreover, the role of non-academic contributors such as citizen scientists [21], [22] and community-based partnerships seems difficult to recognize [23]. Within interdisciplinary projects, other issues such as dissimilar norms in the distribution of authorship credit and author’s order may be present as well. Some fields list authors in alphabetical order and others based on the degree of contribution. It is common in certain disciplines, such as physics, to have hundreds of authors on a paper, whereas in other fields like humanities, one or very few authors may contribute to publications.

### Social challenges and Authorship Criteria
Authorship practices have real consequences, as observed when applying authorship credit for tenure and promotion. While distribution of authorship credit is not straightforward, principles and standards that are suggested for articles involving one or two individuals are similar to articles published by team science endeavours with hundreds or thousands of contributors [24]. To prevent tensions, it is often advised that roles and duties of individuals should be agreed upon and discussed at the outset of a study [17]. However, this can be a challenge as research personnel and the work may change over the course of a project. Furthermore, in most cases explicit discussions about awarding credit occur in response to issues that arise, hence, minimizing the usefulness of discussions [25]. 


### Making contributorship work in systems

To address the need for a more nuanced characterization and contextualization of contributions,
a couple efforts have developed standardized terminologies for contributor roles.
The CRediT taxonomy is a high level standardized vocabulary that contains 14 roles for use in representing scholarly contributions to research outputs [@url:https://casrai.org/credit/; @doi:10.3390/publications7030048].
The Contributor Role Ontology (CRO) was developed as an extension of the CRediT taxonomy, and consumes and expands the contributor roles to provide a structured representation of contribution roles in research and scholarship, which is designed for crediting persons or organizations.
The CRO is an open-source, community-developed ontology that currently contains over 50 classes [@url:https://data2health.github.io/contributor-role-ontology/].
Contribution Role Ontology was developed by the FORCE11 Attribution Working Group.
As noted in the paper by Ilik et al. "this ontology extends the contributions to scholarship beyond manuscript authorship to capture the broadening of researchers’ participation in scientific discoveries that have not been previously recognized by traditional measures of scholarly impact" [@doi:10.3389/frma.2017.00012].
The work done by the FORCE11 Attribution Working Group along with the OpenVIVO Task Force members at the time included reviewing existing scholarly contribution taxonomies and exploring ways to extend the CRediT taxonomy to create a prototype contributorship model that covers a wide selection of fields of research.
The CRO can be used with the Contributor Attribution Model (CAM) - an ontology-based specification for representing information about contributions made to research-related artifacts [@url:https://contributor-attribution-model.readthedocs.io/en/latest/].

### Incentives to use contributions

[**JC: are these potential incentives, or does it really works this way, if the latter, is there data to back it up, if the former, this should be rephrased**]

Benefits to the community could include:
better discovery and selection of team members for team-based science, the creation of a directory of experts that can be used for a variety of needs (treating patients, speaking at a conference or to the media, etc.), impartial selection and nomination for awards and committees, more equitable decisions related to promotion and/or tenure, and the ability to illuminate and eliminate gender disparities in research roles (see citation: @doi:10.1097/ACM.0000000000001261).
Benefits to the Individual include: increased recognition and validation of current skills and expertise, motivation to build upon skills or gain new skills, ease of building your professional brand (i.e. describing or explaining expertise or skills to community), and increased ease of finding and engaging your network of colleagues and experts.

### Expanding measures of success

It should be noted that improving the characterisation/contextualisation of contributions, will not automatically improve the evaluation processes.
The reward system of science has for so long been solely reliant on authorship and the current infrastructure that is used for hiring and promotion is not yet fully prepared to use, and benefit from descriptions of contributions.
As long as scientists are being hired and promoted based on the number of publications, impact factor of journals where they published and positions in the byline, even more accurate identifiers of contributions would not improve scientific evaluation and promotion processes.
Even researchers based in non-academic institutions report similar patterns in evaluation and promotion [@doi:10.1186/1472-6920-10-21].
In other words, as long as institutions have not integrated accurate models of contribution such as CRediT and CRO into their workflows, journals’ adoption alone is not going to benefit the scientific community.

### Manubot Experience and Evaluation

In an ideal world, we would have frictionless workflows that make it easy to introduce credit and attribution to authors and beyond. In addition, workflows would allow seamless ways to promote preservation and discoverability of work.
Tools are being developed that help address this issue, such a [Manubot](https://manubot.org/) [@doi:10/c7np], a workflow and set of tools that promotes open and automated workflows for writing manuscripts.
This commentary was generated using manubot at <https://github.com/data2health/contributorship>.

A great advantage of using Manubot was that is was designed to track author contributions and made the contributions more transparent.
Manubot tracks author metadata in a [YAML file](https://github.com/data2health/contributorship/blob/master/content/metadata.yaml), which can be transfered to other manuscripts, saving time when collecting author details each time you write a manuscript.
We extended the metadata format to include a list of contritubotor roles for each author.
We modified the frontmatter template to note each authors roles along with their identifiers and affiliations, such that readers of the [online manuscript](https://data2health.github.io/contributorship/) have quick access to this information.
The [discussion](https://github.com/data2health/contributorship/issues/5) and [implementation](https://github.com/data2health/contributorship/pull/10) of adding contributor roles are available on GitHub, highlighing another way workflows like Manubot can assist with attribution.
When projects are implemented with full transparency in an online venue, there is a public record of who did what and the intellectual contributions sourrouding decisions.

Manubot incorporates a simple workflow for generating citations from persistent identifiers, to automatically create a reference list and remove the need to use a reference manager.
By removing dependencies on proprietary software and empowering scholars with a cost-free personal preprint server, Manubot can help improve access to scholarly publishing.
On the other hand, tools like Manubot can introduce new barriers that should be considered.
Manubot is more technical to use compared to more commonly used authoring tools like Microsoft Word or Google Docs.
All authors are required to have a GitHub account and some basic familiarity with using GitHub as well as with writing in Markdown.
While Manubot tracked the contributions, the contributions are skewed towards users who are more familiar and comfortable with using GitHub and writing in Markdown.

[**Add something here about using CRO? Manubot should be be able to track our contributions in the yaml file too?**]
[**JC: I am not sure that it actually works, github functionality is only counting commits number, not importance, and manubot would only track work done during manuscript writing. To get something automated we would need something much more complicated, where (for example) data would be published with a list of contributor, that list being imported into the manuscript while linking the manuscript to the data.**]
[**Can it be imported into author submission systems too, so we don't have to fill out the authorship sections in journals? That would be amazing!**]
[**JC: very little portability of the yaml actually, not even to other system using yaml to enter author information. One would need a codec to translate the list from one format to another, see github.com/open-science-promoters/contibutor_manager**]
[**putting citation just with doi has also its disadvantage, I do prefer a solution using a bib file (produced by a reference manager) and an author-date call to the bib file, otherwise one always need to click to see what the publication actually is**]

Some features that would be nice to see in Manubot: [NV: not sure if these already exist? `@dhimmel`]
- A way to tag other users. In google docs, you can +email someone, and they will get an email that you mentioned them in a comment, and can assign them tasks in the doc
- is there spell check integrated somewhere?
- can the yaml file with author metadata be converted to a spreadsheet, for easier viewing? Can the authors be reordered/alphabetized?

| Resource | Link | Function | Open/Closed<br>Data | Data License | Provisions<br>Persistent<br>Identifiers | Persistent<br>Identifiers<br>Used |
|---------------------|----------------------------------|-----------------------------------------------------------------------------------------------------|--------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------|-----------------------------------------|----------------------------------------------------------------------------------|
| CrossRef | https://www.crossref.org | Makes research outputs easy to find, cite, link, assess, and reuse. | Open | CC BY 4.0<br>(https://www.crossref.org/) | Affiliation<br>Funder | Contributor: ORCID<br>Artifact: DOI<br>Funder: Open Funder Registry |
| Open Citations |  |  |  |  |  |  |
| ORCID | https://orcid.org | Provides a persistent scholar identifier that can be used for attribution of any scholarly product. | Open | CC0 for public data file only (https://orcid.org/content/orcid-public-data-file-use-policy). Other content not clear. | Contributor | Contributor: ORCID<br>Artifact: DOI, PubMed ID, PubMed Central ID |
| SemanticScholar | https://www.semanticscholar.org/ | Semantic Scholar is a free, AI-powered search | Open Data <br>(http://api.semanticscholar.org/corpus/) | ODC-BY |  | Contributor<br>Artifact: S2Paper; DOI; ArXivId; MagID; AclId; PubMedID; CorpusID |
| VIAF | http://viaf.org | Name authority service | Open data | ODC-BY | Contributor | Contributor: VIAF<br>Artifact: Worldcat, ISNI, LOC |
| VIVO | https://duraspace.org/vivo | Open source software and ontology representing scholarship. | Open data; Decentralized data |  | Contributor<br>Affiliation<br>Funder | Contributor: VIVO<br>Artifact: DOI, ISBN<br>Affiliation: VIVO<br>Funder: VIVO |
| Wikidata Scholia |  |  |  |  |  |  |
| Dimensions |  |  |  |  |  |  |
| Google Scholar |  |  |  |  |  |  |
| Microsoft Academic |  |  |  |  |  |  |
| Symplectic Elements |  |  |  |  |  |  |
| Academia.edu |  |  |  |  |  |  |
| Meta |  |  |  |  |  |  |
| PublonsID |  |  |  |  |  |  |
| Researchgate |  |  |  |  |  |  |
| Scopus |  |  |  |  |  |  |
